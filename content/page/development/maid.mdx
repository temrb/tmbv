---
title: 'Multi-AI Iterative Development (MAID)'
description: 'An AI-powered framework for systematic software development. Go beyond "vibe coding" to build scalable, maintainable applications.'
icon: 'rotate-left'
iconType: 'solid'
author: 'Temur Bakhriddinov'
date: 2025-08-13
---

## First, what's <span style={{background: 'linear-gradient(45deg, #640D5F, #B12C00, #EB5B00, #FFCC00)', WebkitBackgroundClip: 'text', WebkitTextFillColor: 'transparent', backgroundClip: 'text', fontWeight: 'bold', fontFamily: 'Comic Sans MS, cursive'}}>vibe coding</span> ?

<Frame caption='https://www.reddit.com/r/ProgrammerHumor/comments/1jcjrzf/vibecoding'>
	<img
		src='https://ik.imagekit.io/pqnkhqkwi/tmbv/multi-ai%20iterative%20development/rc5kiucui1pe1.jpeg?updatedAt=1755063956635'
		aria-label='A humorous take on vibe coding'
		alt='A humorous take on vibe coding'
	/>
</Frame>

**Vibe coding** is when you jump straight into building through AI prompting with no plan—just a feeling. It feels fast, but this path almost always leads to a fragile foundation that cracks under pressure.

Without a clear blueprint, the AI develops _amnesia_, changing styles and logic mid-project. This inevitably leads to scope creep, where the original problem gets lost in a half-finished product riddled with disorganized code and unexpected bugs.

Instead of building on a vibe, let's build from a blueprint. The following steps provide the clarity and structure needed to create robust, scalable applications.

<CardGroup cols={2}>
	<Card
		title='Conversational Research'
		icon='circle-1'
		href='#conversational-research'
	>
		Define project goals and "why" with Socratic dialogue.
	</Card>
	<Card title='Build Context' icon='circle-2' href='#build-context-library'>
		Create the complete project blueprint before you code.
	</Card>
	<Card title='Implement' icon='circle-3' href='#implement-functionality'>
		Translate the blueprint into clean, validated code.
	</Card>
	<Card title='Debug Issues' icon='circle-4' href='#debug-issues'>
		Solve root causes with precise, targeted context.
	</Card>
</CardGroup>

## Conversational Research

The first and most critical step is **dialogue**. This isn't a casual chat; it's a structured conversation that prevents "vibe coding" by moving from a high-level vision to a concrete technical path. This phase has two parts: a flexible **Socratic Funnel** to define your goals, followed by **Deep Research** to validate the technical approach.

This process adapts to you. A non-technical founder might spend more time on the _Why_, while a senior engineer might focus on the _How_. The goal is the same: clarity.

### The Socratic Funnel

You'll use an AI like [ChatGPT](https://chat.openai.com/) as a partner to progress through three distinct stages.

<Steps>
  <Step title='The "Why" (High-Level Vision)'>
    Focus entirely on the human element. This stage is about defining the user's pain point and the ultimate goal, using simple, non-technical language.

    **Key Questions to Explore:**
    - Who is this for?
    - Is this an internal tool or a public-facing product?
    - What specific problem does this solve for them?
    - What does success look like from their perspective?

  </Step>
  <Step title='The "What" (Functional Requirements)'>
    Translate the vision into tangible features and behaviors. You're defining what the system must *do* without yet deciding on the technology.

    **Key Questions to Explore:**
    - What are the core features needed for an MVP?
    - What kind of data will the system handle?
    - Are there critical needs like speed, security, or scale?

  </Step>
  <Step title='The "How" (Technical Exploration)'>
    Once the "what" is clear, you can explore the "how." This is where a non-technical person can ask the AI for suggestions, and a technical person can validate their ideas.

    **Key Questions to Explore:**
    - Based on our needs, what technologies are a good fit?
    - What are the trade-offs between different frameworks?
    - What would a basic system architecture look like?

  </Step>
</Steps>

<Info>
	The key is to address the stages in order, ensuring your technical decisions
	are driven by a clear purpose. Move through the funnel at your own pace.
</Info>

### Deep Research

With a list of potential technologies from "The How," you're ready to transition from conversation to documentation. Task the AI with conducting **deep research** to generate a formal analysis report. This deliverable should outline all viable tools and libraries, provide a comparison matrix of pros and cons for your use case, offer a clear recommendation, and discuss any integration considerations that might affect the overall architecture.

### Scenario

Let's see how this research phase works for a non-technical user who wants to build an `internal web scraping tool`.

<AccordionGroup>
  <Accordion title="The dialogue begins with a vague idea..." icon="user">
    **User:** "I need a way to get data from websites."
  </Accordion>

<Accordion title='The AI probes for purpose' icon='robot'>
	**AI:** "Okay, let's explore that. What's the main goal? What will you
	**do** with this data once you have it?"
</Accordion>

<Accordion title='The user reveals the core motivation' icon='user'>
	**User:** "I want to feed it into another AI to train a custom model for my
	business. The data needs to be high quality."
</Accordion>

<Accordion title='The AI defines functional needs...' icon='robot'>
	**AI:** "Got it. That means accuracy and structure are key. What kind of
	websites are we talking about? Simple articles, or complex pages like
	e-commerce sites where you have to click things to see information?"
</Accordion>

<Accordion title='...and the user provides critical details' icon='user'>
	**User:** "Both, actually. Some are simple blogs, but others are product
	pages where prices and reviews appear dynamically."
</Accordion>

<Accordion title='The AI synthesizes and suggests a path forward' icon='robot'>
	**AI:** "Perfect. So we need an internal tool that can scrape both static
	and dynamic sites and output clean, structured data. Based on that, we
	should explore technologies that can control a browser, like **Playwright**,
	combined with a library like **Pydantic** to enforce data structure. Does
	this sound like the right direction?"
</Accordion>

  <Accordion title="The user asks the final, informed question" icon="user">
    **User:** "Yes, that's exactly it. What are the pros and cons of those tools you mentioned compared to others?"
  </Accordion>
</AccordionGroup>

### In Essence...

That final question is the perfect outcome of the Socratic funnel. The user started with a vague idea ("get data") and ended with a specific, well-defined query ("pros and cons of these tools"). This question now becomes the direct prompt for the AI's analysis report.

**Example AI Report:**

```yaml
Tool Analysis Report:
    - BeautifulSoup: Best for simple HTML parsing. Fast and lightweight.
    - Scrapy: Excellent for large-scale, production-level scraping.
    - Playwright: Ideal for modern, JavaScript-heavy sites.
    - Selenium: Good for complex browser interactions but can be slower.

Recommendation: Playwright + structured output with Pydantic.
Reasoning: Handles modern web apps, offers fast execution, and ensures type-safe outputs for reliable data processing.
```

## Build Context Library

With your research complete, it's time to build the project's **context library**. This is the most critical step in the MAID framework. It’s the official, machine-readable blueprint that acts as a single source of truth for your AI collaborator.

A well-built context library prevents AI amnesia. Instead of making decisions on the fly, the AI will consistently refer to this library, ensuring the code it produces aligns perfectly with your project's rules and architecture.

<Tabs>
  <Tab title="Document Indexing" icon="book">
    First, you must provide the AI with the official documentation for the technologies you chose during research. If the AI doesn't have the manufacturer's instructions, it will invent its own, often leading to bugs and deprecated code.

    For our web scraper, we need the docs for `Playwright` and `Pydantic`. You have two excellent options for this:

    **Deep Control (gpt-crawler)**

    For maximum control, use a tool like [gpt-crawler](https://github.com/BuilderIO/gpt-crawler) to crawl and index the documentation websites yourself. This creates a local, structured JSON file of the content.

    ```bash
    # Crawl Playwright's Python docs
    npx gpt-crawler --url https://playwright.dev/python/docs/intro --output playwright-docs.json

    # Crawl Pydantic's docs
    npx gpt-crawler --url https://docs.pydantic.dev/latest/ --output pydantic-docs.json
    ```

    **Quick Start (Context7)**

    For a faster, no-code approach, a service like [Context7](https://context7.com) (available as MCP) allows you to simply paste the documentation URLs. It handles the crawling and provides a ready-to-use context package for your AI.

  </Tab>
  <Tab title="Project Rules" icon="shield">
    Next, you create a project rules file. This is where you enforce your architectural decisions and coding standards. Think of it as the list of non-negotiable laws for your project that the AI must always obey.

    **For Claude Code Users:**

    Create a `CLAUDE.md` file in your project root. This follows [Claude Code's memory management system](https://docs.anthropic.com/en/docs/claude-code/memory) and provides hierarchical project instructions that Claude automatically loads.

    **For Other AI Tools:**

    Create a `PROJECT_RULES.md` file with similar structured instructions for your chosen AI assistant.

    This document prevents the AI from deviating from your plan. It should be specific and unambiguous, using structured markdown with clear bullet points.

  </Tab>
</Tabs>

### Scenario

Continuing with our web scraping tool, we'll create a `CLAUDE.md` file that reflects our findings from the Research phase.

**`CLAUDE.md`:**

```markdown
# Project: Internal Web Scraper

## 1. Architecture Decisions

- **Core Framework**: Playwright for browser automation.
- **Data Structuring**: Pydantic models for all scraped data.
- **Application Type**: Asynchronous command-line tool.

## 2. Non-Negotiable Rules

- All web scraping must respect `robots.txt` directives.
- Implement a 1-second delay between requests to the same domain.
- Use structured logging (JSON format) for all errors and warnings.

## 3. Code Standards

- All functions must include type hints.
- Public functions require a docstring explaining their purpose, arguments, and return value.
- Secrets or API keys must never be hardcoded.
```

<Tip>
	Your project rules file should be a living document. As you make new
	architectural decisions during the project's lifecycle, update them here
	first.
</Tip>

With the `Playwright` and `Pydantic` documentation indexed and the project rules defined in `CLAUDE.md`, our Context Library is complete. We now have a comprehensive blueprint ready to hand off for implementation, ensuring the AI builds exactly what we planned.
